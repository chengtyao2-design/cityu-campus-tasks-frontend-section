#!/usr/bin/env python3
"""
FAISS ç´¢å¼•æ„å»ºè„šæœ¬
ä»çŸ¥è¯†åº“æ•°æ®æ„å»ºå‘é‡ç´¢å¼•ï¼Œæ”¯æŒ CLI ä½¿ç”¨å’Œæµ‹è¯•
"""

import sys
import os
import argparse
import logging
import json
from pathlib import Path
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from app.services.embedder import EmbeddingService, create_embedding_service
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€éœ€ä¾èµ–: pip install sentence-transformers faiss-cpu numpy")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class IndexBuilder:
    """ç´¢å¼•æ„å»ºå™¨"""
    
    def __init__(self, model_name: str = "BAAI/bge-small-zh-v1.5", 
                 chunk_size: int = 550, overlap: int = 100):
        """
        åˆå§‹åŒ–ç´¢å¼•æ„å»ºå™¨
        
        Args:
            model_name: åµŒå…¥æ¨¡å‹åç§°
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å° (400-700)
            overlap: é‡å å¤§å° (80-120)
        """
        self.service = create_embedding_service(
            model_name=model_name,
            chunk_size=chunk_size,
            overlap=overlap
        )
        logger.info(f"ç´¢å¼•æ„å»ºå™¨åˆå§‹åŒ–: model={model_name}, chunk_size={chunk_size}, overlap={overlap}")
    
    def load_knowledge_data(self, data_path: str) -> List[Dict[str, Any]]:
        """
        åŠ è½½çŸ¥è¯†åº“æ•°æ®
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„ (æ”¯æŒ .jsonl å’Œ .json)
            
        Returns:
            List[Dict]: çŸ¥è¯†åº“æ•°æ®åˆ—è¡¨
        """
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        data = []
        
        if data_path.endswith('.jsonl'):
            # è¯»å– JSONL æ ¼å¼
            with open(data_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        try:
                            item = json.loads(line)
                            data.append(item)
                        except json.JSONDecodeError as e:
                            logger.warning(f"ç¬¬ {line_num} è¡Œ JSON è§£æå¤±è´¥: {e}")
        
        elif data_path.endswith('.json'):
            # è¯»å– JSON æ ¼å¼
            with open(data_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
                if isinstance(json_data, list):
                    data = json_data
                else:
                    data = [json_data]
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {data_path}")
        
        logger.info(f"åŠ è½½çŸ¥è¯†åº“æ•°æ®: {len(data)} æ¡è®°å½•")
        return data
    
    def extract_texts_from_knowledge(self, knowledge_data: List[Dict[str, Any]]) -> tuple:
        """
        ä»çŸ¥è¯†åº“æ•°æ®ä¸­æå–æ–‡æœ¬
        
        Args:
            knowledge_data: çŸ¥è¯†åº“æ•°æ®åˆ—è¡¨
            
        Returns:
            tuple: (texts, sources, metadata_list)
        """
        texts = []
        sources = []
        metadata_list = []
        
        for item in knowledge_data:
            # æå–æ–‡æœ¬å†…å®¹
            text_content = ""
            if 'content' in item:
                text_content = str(item['content'])
            elif 'text' in item:
                text_content = str(item['text'])
            elif 'description' in item:
                text_content = str(item['description'])
            else:
                # å°è¯•ä»å…¶ä»–å­—æ®µæå–æ–‡æœ¬
                for key, value in item.items():
                    if isinstance(value, str) and len(value) > 20:
                        text_content = value
                        break
            
            if not text_content.strip():
                logger.warning(f"è·³è¿‡ç©ºæ–‡æœ¬è®°å½•: {item}")
                continue
            
            # æå–æ¥æº
            source = item.get('task_id', item.get('id', item.get('source', 'unknown')))
            
            # æå–å…ƒæ•°æ®
            metadata = {k: v for k, v in item.items() if k not in ['content', 'text', 'description']}
            
            texts.append(text_content)
            sources.append(str(source))
            metadata_list.append(metadata)
        
        logger.info(f"æå–æ–‡æœ¬å®Œæˆ: {len(texts)} ä¸ªæœ‰æ•ˆæ–‡æœ¬")
        return texts, sources, metadata_list
    
    def build_index(self, data_path: str, output_path: str = "data/index") -> str:
        """
        æ„å»ºç´¢å¼•
        
        Args:
            data_path: çŸ¥è¯†åº“æ•°æ®è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„ (ä¸å«æ‰©å±•å)
            
        Returns:
            str: ç´¢å¼•æ–‡ä»¶è·¯å¾„
        """
        logger.info(f"å¼€å§‹æ„å»ºç´¢å¼•: {data_path} -> {output_path}")
        
        # åŠ è½½æ•°æ®
        knowledge_data = self.load_knowledge_data(data_path)
        
        # æå–æ–‡æœ¬
        texts, sources, metadata_list = self.extract_texts_from_knowledge(knowledge_data)
        
        if not texts:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®")
        
        # æ„å»ºç´¢å¼•
        logger.info("å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")
        index = self.service.build_index_from_texts(texts, sources, metadata_list)
        
        # ä¿å­˜ç´¢å¼•
        output_dir = os.path.dirname(output_path) if os.path.dirname(output_path) else "."
        os.makedirs(output_dir, exist_ok=True)
        index_path, metadata_path = self.service.save_index(output_path)
        
        logger.info(f"ç´¢å¼•æ„å»ºå®Œæˆ: {index_path}")
        return index_path
    
    def test_index(self, index_path: str, test_queries: List[str] = None, 
                   top_k: int = 4, min_similarity: float = 0.35) -> Dict[str, Any]:
        """
        æµ‹è¯•ç´¢å¼•
        
        Args:
            index_path: ç´¢å¼•è·¯å¾„ (ä¸å«æ‰©å±•å)
            test_queries: æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
            top_k: è¿”å›ç»“æœæ•°é‡
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            Dict: æµ‹è¯•ç»“æœ
        """
        logger.info(f"å¼€å§‹æµ‹è¯•ç´¢å¼•: {index_path}")
        
        # åŠ è½½ç´¢å¼•
        self.service.load_index(index_path)
        
        # é»˜è®¤æµ‹è¯•æŸ¥è¯¢
        if test_queries is None:
            test_queries = [
                "å›¾ä¹¦é¦†åœ¨å“ªé‡Œ",
                "å¦‚ä½•ä½¿ç”¨å®éªŒå®¤è®¾å¤‡",
                "å­¦ç”Ÿæ´»åŠ¨ä¸­å¿ƒçš„å¼€æ”¾æ—¶é—´",
                "è®¡ç®—æœºç§‘å­¦è¯¾ç¨‹",
                "æ ¡å›­å®‰å…¨è§„å®š"
            ]
        
        test_results = {
            'total_queries': len(test_queries),
            'successful_queries': 0,
            'average_similarity': 0.0,
            'results': []
        }
        
        total_similarity = 0.0
        successful_count = 0
        
        for query in test_queries:
            logger.info(f"æµ‹è¯•æŸ¥è¯¢: {query}")
            
            try:
                results = self.service.search(query, top_k=top_k, min_similarity=min_similarity)
                
                query_result = {
                    'query': query,
                    'results_count': len(results),
                    'max_similarity': max([r.similarity for r in results]) if results else 0.0,
                    'results': [
                        {
                            'text': r.text[:200] + '...' if len(r.text) > 200 else r.text,
                            'similarity': r.similarity,
                            'source': r.source
                        }
                        for r in results
                    ]
                }
                
                test_results['results'].append(query_result)
                
                if results:
                    successful_count += 1
                    total_similarity += query_result['max_similarity']
                    
                    print(f"âœ… æŸ¥è¯¢: {query}")
                    print(f"   æ‰¾åˆ° {len(results)} ä¸ªç»“æœ, æœ€é«˜ç›¸ä¼¼åº¦: {query_result['max_similarity']:.3f}")
                    for i, result in enumerate(results[:2], 1):
                        print(f"   [{i}] {result.similarity:.3f} - {result.text[:100]}...")
                else:
                    print(f"âŒ æŸ¥è¯¢: {query} - æœªæ‰¾åˆ°æ»¡è¶³é˜ˆå€¼çš„ç»“æœ")
                
                print()
                
            except Exception as e:
                logger.error(f"æŸ¥è¯¢å¤±è´¥: {query}, é”™è¯¯: {str(e)}")
                query_result = {
                    'query': query,
                    'error': str(e),
                    'results_count': 0,
                    'max_similarity': 0.0,
                    'results': []
                }
                test_results['results'].append(query_result)
        
        test_results['successful_queries'] = successful_count
        test_results['average_similarity'] = total_similarity / successful_count if successful_count > 0 else 0.0
        
        # è¾“å‡ºæµ‹è¯•æ€»ç»“
        print("ğŸ“Š æµ‹è¯•æ€»ç»“")
        print("=" * 50)
        print(f"æ€»æŸ¥è¯¢æ•°: {test_results['total_queries']}")
        print(f"æˆåŠŸæŸ¥è¯¢æ•°: {test_results['successful_queries']}")
        print(f"æˆåŠŸç‡: {test_results['successful_queries'] / test_results['total_queries'] * 100:.1f}%")
        print(f"å¹³å‡ç›¸ä¼¼åº¦: {test_results['average_similarity']:.3f}")
        print(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {min_similarity}")
        
        # éªŒè¯æ˜¯å¦æ»¡è¶³è¦æ±‚
        meets_requirements = (
            test_results['successful_queries'] > 0 and
            test_results['average_similarity'] >= min_similarity
        )
        
        if meets_requirements:
            print("âœ… æµ‹è¯•é€šè¿‡: æ»¡è¶³ç›¸ä¼¼åº¦è¦æ±‚")
        else:
            print("âŒ æµ‹è¯•æœªé€šè¿‡: æœªæ»¡è¶³ç›¸ä¼¼åº¦è¦æ±‚")
        
        return test_results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="FAISS ç´¢å¼•æ„å»ºå’Œæµ‹è¯•å·¥å…·")
    
    parser.add_argument('--data', '-d', required=True, 
                       help='çŸ¥è¯†åº“æ•°æ®æ–‡ä»¶è·¯å¾„ (.json æˆ– .jsonl)')
    parser.add_argument('--output', '-o', default='data/index',
                       help='è¾“å‡ºç´¢å¼•è·¯å¾„ (ä¸å«æ‰©å±•å, é»˜è®¤: data/index)')
    parser.add_argument('--model', '-m', default='BAAI/bge-small-zh-v1.5',
                       help='åµŒå…¥æ¨¡å‹åç§° (é»˜è®¤: BAAI/bge-small-zh-v1.5)')
    parser.add_argument('--chunk-size', type=int, default=550,
                       help='æ–‡æœ¬åˆ†å—å¤§å° (400-700, é»˜è®¤: 550)')
    parser.add_argument('--overlap', type=int, default=100,
                       help='åˆ†å—é‡å å¤§å° (80-120, é»˜è®¤: 100)')
    parser.add_argument('--test', action='store_true',
                       help='æ„å»ºåè¿›è¡Œæµ‹è¯•')
    parser.add_argument('--test-only', action='store_true',
                       help='ä»…æµ‹è¯•ç°æœ‰ç´¢å¼• (éœ€è¦ --index å‚æ•°)')
    parser.add_argument('--index', help='ç°æœ‰ç´¢å¼•è·¯å¾„ (ç”¨äº --test-only)')
    parser.add_argument('--top-k', type=int, default=4,
                       help='æœç´¢è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 4)')
    parser.add_argument('--min-similarity', type=float, default=0.35,
                       help='æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼ (é»˜è®¤: 0.35)')
    parser.add_argument('--queries', nargs='+',
                       help='è‡ªå®šä¹‰æµ‹è¯•æŸ¥è¯¢')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # éªŒè¯å‚æ•°
    if args.chunk_size < 400 or args.chunk_size > 700:
        print("é”™è¯¯: chunk-size å¿…é¡»åœ¨ 400-700 ä¹‹é—´")
        sys.exit(1)
    
    if args.overlap < 80 or args.overlap > 120:
        print("é”™è¯¯: overlap å¿…é¡»åœ¨ 80-120 ä¹‹é—´")
        sys.exit(1)
    
    try:
        # åˆ›å»ºç´¢å¼•æ„å»ºå™¨
        builder = IndexBuilder(
            model_name=args.model,
            chunk_size=args.chunk_size,
            overlap=args.overlap
        )
        
        if args.test_only:
            # ä»…æµ‹è¯•æ¨¡å¼
            if not args.index:
                print("é”™è¯¯: --test-only æ¨¡å¼éœ€è¦ --index å‚æ•°")
                sys.exit(1)
            
            test_results = builder.test_index(
                args.index,
                test_queries=args.queries,
                top_k=args.top_k,
                min_similarity=args.min_similarity
            )
            
        else:
            # æ„å»ºç´¢å¼•
            print(f"ğŸ”§ å¼€å§‹æ„å»ºç´¢å¼•")
            print(f"æ•°æ®æ–‡ä»¶: {args.data}")
            print(f"è¾“å‡ºè·¯å¾„: {args.output}")
            print(f"æ¨¡å‹: {args.model}")
            print(f"åˆ†å—å¤§å°: {args.chunk_size}, é‡å : {args.overlap}")
            print("=" * 60)
            
            index_path = builder.build_index(args.data, args.output)
            
            print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {index_path}")
            
            # å¯é€‰æµ‹è¯•
            if args.test:
                print("\nğŸ§ª å¼€å§‹æµ‹è¯•ç´¢å¼•...")
                test_results = builder.test_index(
                    args.output,
                    test_queries=args.queries,
                    top_k=args.top_k,
                    min_similarity=args.min_similarity
                )
        
        print("\nğŸ‰ æ“ä½œå®Œæˆ!")
        
    except Exception as e:
        logger.error(f"æ“ä½œå¤±è´¥: {str(e)}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()