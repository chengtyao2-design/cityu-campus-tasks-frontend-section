#!/usr/bin/env python3
"""
å‘é‡åµŒå…¥æœåŠ¡æ¨¡å—
æä¾›æ–‡æœ¬åˆ†å—ã€BGE-small-zh åµŒå…¥å’Œ FAISS ç´¢å¼•åŠŸèƒ½
"""

import os
import json
import logging
import numpy as np
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass, asdict
import faiss
from sentence_transformers import SentenceTransformer
import re
from pathlib import Path

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

@dataclass
class TextChunk:
    """æ–‡æœ¬å—æ•°æ®ç±»"""
    chunk_id: str
    text: str
    source: str
    metadata: Dict[str, Any]
    start_pos: int = 0
    end_pos: int = 0

@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç±»"""
    chunk_id: str
    text: str
    similarity: float
    source: str
    metadata: Dict[str, Any]

class TextChunker:
    """æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, chunk_size: int = 550, overlap: int = 100):
        """
        åˆå§‹åŒ–æ–‡æœ¬åˆ†å—å™¨
        
        Args:
            chunk_size: åˆ†å—å¤§å° (400-700)
            overlap: é‡å å¤§å° (80-120)
        """
        self.chunk_size = max(400, min(700, chunk_size))
        self.overlap = max(80, min(120, overlap))
        logger.info(f"æ–‡æœ¬åˆ†å—å™¨åˆå§‹åŒ–: chunk_size={self.chunk_size}, overlap={self.overlap}")
    
    def chunk_text(self, text: str, source: str = "", metadata: Dict[str, Any] = None) -> List[TextChunk]:
        """
        å°†æ–‡æœ¬åˆ†å—
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            source: æ–‡æœ¬æ¥æº
            metadata: å…ƒæ•°æ®
            
        Returns:
            List[TextChunk]: æ–‡æœ¬å—åˆ—è¡¨
        """
        if not text or not text.strip():
            return []
        
        if metadata is None:
            metadata = {}
        
        text = text.strip()
        chunks = []
        
        # å¦‚æœæ–‡æœ¬é•¿åº¦å°äºåˆ†å—å¤§å°ï¼Œç›´æ¥è¿”å›
        if len(text) <= self.chunk_size:
            chunk = TextChunk(
                chunk_id=f"{source}_0",
                text=text,
                source=source,
                metadata=metadata,
                start_pos=0,
                end_pos=len(text)
            )
            chunks.append(chunk)
            return chunks
        
        # åˆ†å—å¤„ç†
        start = 0
        chunk_index = 0
        
        while start < len(text):
            end = start + self.chunk_size
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨å¥å­è¾¹ç•Œåˆ†å‰²
            if end < len(text):
                # å¯»æ‰¾å¥å­ç»“æŸç¬¦
                sentence_ends = []
                for i, char in enumerate(text[start:end]):
                    if char in 'ã€‚ï¼ï¼Ÿ.!?':
                        sentence_ends.append(start + i + 1)
                
                # å¦‚æœæ‰¾åˆ°å¥å­è¾¹ç•Œï¼Œåœ¨æœ€åä¸€ä¸ªå¥å­è¾¹ç•Œåˆ†å‰²
                if sentence_ends:
                    end = sentence_ends[-1]
                # å¦åˆ™å¯»æ‰¾æ ‡ç‚¹ç¬¦å·
                elif 'ï¼Œ' in text[start:end] or ',' in text[start:end]:
                    for i in range(end - start - 1, -1, -1):
                        if text[start + i] in 'ï¼Œ,':
                            end = start + i + 1
                            break
                # æœ€åå¯»æ‰¾ç©ºæ ¼
                elif ' ' in text[start:end]:
                    for i in range(end - start - 1, -1, -1):
                        if text[start + i] == ' ':
                            end = start + i
                            break
            
            chunk_text = text[start:end].strip()
            if chunk_text:
                chunk = TextChunk(
                    chunk_id=f"{source}_{chunk_index}",
                    text=chunk_text,
                    source=source,
                    metadata=metadata,
                    start_pos=start,
                    end_pos=end
                )
                chunks.append(chunk)
                chunk_index += 1
            
            # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            start = max(start + 1, end - self.overlap)
            
            # é¿å…æ— é™å¾ªç¯
            if start >= len(text):
                break
        
        logger.debug(f"æ–‡æœ¬åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—, æ¥æº: {source}")
        return chunks

class BGEEmbedder:
    """BGE-small-zh åµŒå…¥æ¨¡å‹"""
    
    def __init__(self, model_name: str = "BAAI/bge-small-zh-v1.5"):
        """
        åˆå§‹åŒ– BGE åµŒå…¥æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.model = None
        self.embedding_dim = 512  # BGE-small-zh çš„åµŒå…¥ç»´åº¦
        
    def load_model(self):
        """åŠ è½½åµŒå…¥æ¨¡å‹"""
        try:
            logger.info(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
            self.model = SentenceTransformer(self.model_name)
            self.embedding_dim = self.model.get_sentence_embedding_dimension()
            logger.info(f"åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ, ç»´åº¦: {self.embedding_dim}")
        except Exception as e:
            logger.error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            # å›é€€åˆ°ç®€å•çš„æ¨¡å‹
            logger.info("å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹...")
            try:
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
                self.embedding_dim = self.model.get_sentence_embedding_dimension()
                logger.info(f"å¤‡ç”¨æ¨¡å‹åŠ è½½æˆåŠŸ, ç»´åº¦: {self.embedding_dim}")
            except Exception as e2:
                logger.error(f"å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {str(e2)}")
                raise e2
    
    def encode_texts(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """
        ç¼–ç æ–‡æœ¬ä¸ºå‘é‡
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            np.ndarray: åµŒå…¥å‘é‡çŸ©é˜µ
        """
        if self.model is None:
            self.load_model()
        
        if not texts:
            return np.array([]).reshape(0, self.embedding_dim)
        
        try:
            logger.debug(f"å¼€å§‹ç¼–ç  {len(texts)} ä¸ªæ–‡æœ¬")
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=len(texts) > 100,
                convert_to_numpy=True,
                normalize_embeddings=True  # å½’ä¸€åŒ–åµŒå…¥å‘é‡
            )
            logger.debug(f"æ–‡æœ¬ç¼–ç å®Œæˆ, å½¢çŠ¶: {embeddings.shape}")
            return embeddings
        except Exception as e:
            logger.error(f"æ–‡æœ¬ç¼–ç å¤±è´¥: {str(e)}")
            raise

class FAISSIndex:
    """FAISS å‘é‡ç´¢å¼•"""
    
    def __init__(self, embedding_dim: int = 512):
        """
        åˆå§‹åŒ– FAISS ç´¢å¼•
        
        Args:
            embedding_dim: åµŒå…¥å‘é‡ç»´åº¦
        """
        self.embedding_dim = embedding_dim
        self.index = None
        self.chunks = []  # å­˜å‚¨æ–‡æœ¬å—
        self.chunk_id_to_idx = {}  # chunk_id åˆ°ç´¢å¼•çš„æ˜ å°„
        
    def build_index(self, chunks: List[TextChunk], embeddings: np.ndarray):
        """
        æ„å»º FAISS ç´¢å¼•
        
        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨
            embeddings: åµŒå…¥å‘é‡çŸ©é˜µ
        """
        if len(chunks) != len(embeddings):
            raise ValueError(f"æ–‡æœ¬å—æ•°é‡ ({len(chunks)}) ä¸åµŒå…¥å‘é‡æ•°é‡ ({len(embeddings)}) ä¸åŒ¹é…")
        
        logger.info(f"å¼€å§‹æ„å»º FAISS ç´¢å¼•: {len(chunks)} ä¸ªå‘é‡, ç»´åº¦: {self.embedding_dim}")
        
        # åˆ›å»º FAISS ç´¢å¼• (ä½¿ç”¨ L2 è·ç¦»)
        self.index = faiss.IndexFlatL2(self.embedding_dim)
        
        # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        if len(embeddings) > 0:
            embeddings = embeddings.astype(np.float32)
            self.index.add(embeddings)
        
        # å­˜å‚¨æ–‡æœ¬å—å’Œæ˜ å°„
        self.chunks = chunks
        self.chunk_id_to_idx = {chunk.chunk_id: i for i, chunk in enumerate(chunks)}
        
        logger.info(f"FAISS ç´¢å¼•æ„å»ºå®Œæˆ: {self.index.ntotal} ä¸ªå‘é‡")
    
    def search(self, query_embedding: np.ndarray, top_k: int = 4, min_similarity: float = 0.35) -> List[SearchResult]:
        """
        æœç´¢ç›¸ä¼¼å‘é‡
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›ç»“æœæ•°é‡
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            List[SearchResult]: æœç´¢ç»“æœåˆ—è¡¨
        """
        if self.index is None or self.index.ntotal == 0:
            logger.warning("ç´¢å¼•ä¸ºç©ºæˆ–æœªæ„å»º")
            return []
        
        query_embedding = query_embedding.astype(np.float32).reshape(1, -1)
        
        # æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡
        distances, indices = self.index.search(query_embedding, min(top_k, self.index.ntotal))
        
        results = []
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx == -1:  # FAISS è¿”å› -1 è¡¨ç¤ºæ— æ•ˆç»“æœ
                continue
            
            # å°† L2 è·ç¦»è½¬æ¢ä¸ºä½™å¼¦ç›¸ä¼¼åº¦
            # ç”±äºå‘é‡å·²å½’ä¸€åŒ–ï¼ŒL2è·ç¦» = 2 * (1 - ä½™å¼¦ç›¸ä¼¼åº¦)
            similarity = max(0.0, 1.0 - distance / 2.0)
            
            if similarity >= min_similarity:
                chunk = self.chunks[idx]
                result = SearchResult(
                    chunk_id=chunk.chunk_id,
                    text=chunk.text,
                    similarity=similarity,
                    source=chunk.source,
                    metadata=chunk.metadata
                )
                results.append(result)
        
        logger.debug(f"æœç´¢å®Œæˆ: è¿”å› {len(results)} ä¸ªç»“æœ (é˜ˆå€¼: {min_similarity})")
        return results
    
    def save_index(self, index_path: str, metadata_path: str):
        """
        ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶
        
        Args:
            index_path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
            metadata_path: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»ºï¼Œæ— æ³•ä¿å­˜")
        
        # ä¿å­˜ FAISS ç´¢å¼•
        faiss.write_index(self.index, index_path)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'embedding_dim': self.embedding_dim,
            'total_vectors': self.index.ntotal,
            'chunks': [asdict(chunk) for chunk in self.chunks],
            'chunk_id_to_idx': self.chunk_id_to_idx
        }
        
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ç´¢å¼•å·²ä¿å­˜: {index_path}, å…ƒæ•°æ®: {metadata_path}")
    
    def load_index(self, index_path: str, metadata_path: str):
        """
        ä»æ–‡ä»¶åŠ è½½ç´¢å¼•
        
        Args:
            index_path: ç´¢å¼•æ–‡ä»¶è·¯å¾„
            metadata_path: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
        """
        if not os.path.exists(index_path) or not os.path.exists(metadata_path):
            raise FileNotFoundError(f"ç´¢å¼•æ–‡ä»¶æˆ–å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
        
        # åŠ è½½ FAISS ç´¢å¼•
        self.index = faiss.read_index(index_path)
        
        # åŠ è½½å…ƒæ•°æ®
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        self.embedding_dim = metadata['embedding_dim']
        self.chunk_id_to_idx = metadata['chunk_id_to_idx']
        
        # é‡å»ºæ–‡æœ¬å—å¯¹è±¡
        self.chunks = []
        for chunk_data in metadata['chunks']:
            chunk = TextChunk(**chunk_data)
            self.chunks.append(chunk)
        
        logger.info(f"ç´¢å¼•å·²åŠ è½½: {self.index.ntotal} ä¸ªå‘é‡, ç»´åº¦: {self.embedding_dim}")

class EmbeddingService:
    """åµŒå…¥æœåŠ¡ä¸»ç±»"""
    
    def __init__(self, model_name: str = "BAAI/bge-small-zh-v1.5", chunk_size: int = 550, overlap: int = 100):
        """
        åˆå§‹åŒ–åµŒå…¥æœåŠ¡
        
        Args:
            model_name: åµŒå…¥æ¨¡å‹åç§°
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            overlap: åˆ†å—é‡å å¤§å°
        """
        self.chunker = TextChunker(chunk_size=chunk_size, overlap=overlap)
        self.embedder = BGEEmbedder(model_name=model_name)
        self.index = None
        
        logger.info("åµŒå…¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
    
    def build_index_from_texts(self, texts: List[str], sources: List[str] = None, 
                              metadata_list: List[Dict[str, Any]] = None) -> FAISSIndex:
        """
        ä»æ–‡æœ¬åˆ—è¡¨æ„å»ºç´¢å¼•
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            sources: æ¥æºåˆ—è¡¨
            metadata_list: å…ƒæ•°æ®åˆ—è¡¨
            
        Returns:
            FAISSIndex: æ„å»ºçš„ç´¢å¼•
        """
        if not texts:
            raise ValueError("æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
        
        if sources is None:
            sources = [f"text_{i}" for i in range(len(texts))]
        
        if metadata_list is None:
            metadata_list = [{}] * len(texts)
        
        # æ–‡æœ¬åˆ†å—
        all_chunks = []
        for i, text in enumerate(texts):
            source = sources[i] if i < len(sources) else f"text_{i}"
            metadata = metadata_list[i] if i < len(metadata_list) else {}
            chunks = self.chunker.chunk_text(text, source, metadata)
            all_chunks.extend(chunks)
        
        logger.info(f"æ–‡æœ¬åˆ†å—å®Œæˆ: {len(all_chunks)} ä¸ªå—")
        
        # æå–æ–‡æœ¬è¿›è¡ŒåµŒå…¥
        chunk_texts = [chunk.text for chunk in all_chunks]
        embeddings = self.embedder.encode_texts(chunk_texts)
        
        # æ„å»ºç´¢å¼•
        self.index = FAISSIndex(embedding_dim=self.embedder.embedding_dim)
        self.index.build_index(all_chunks, embeddings)
        
        return self.index
    
    def search(self, query: str, top_k: int = 4, min_similarity: float = 0.35) -> List[SearchResult]:
        """
        æœç´¢ç›¸ä¼¼æ–‡æœ¬
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            List[SearchResult]: æœç´¢ç»“æœ
        """
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨ build_index_from_texts")
        
        # ç¼–ç æŸ¥è¯¢æ–‡æœ¬
        query_embedding = self.embedder.encode_texts([query])
        
        # æœç´¢
        return self.index.search(query_embedding[0], top_k=top_k, min_similarity=min_similarity)
    
    def save_index(self, base_path: str):
        """
        ä¿å­˜ç´¢å¼•
        
        Args:
            base_path: åŸºç¡€è·¯å¾„ (ä¸å«æ‰©å±•å)
        """
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»ºï¼Œæ— æ³•ä¿å­˜")
        
        index_path = f"{base_path}.faiss"
        metadata_path = f"{base_path}.json"
        
        self.index.save_index(index_path, metadata_path)
        return index_path, metadata_path
    
    def load_index(self, base_path: str):
        """
        åŠ è½½ç´¢å¼•
        
        Args:
            base_path: åŸºç¡€è·¯å¾„ (ä¸å«æ‰©å±•å)
        """
        index_path = f"{base_path}.faiss"
        metadata_path = f"{base_path}.json"
        
        self.index = FAISSIndex()
        self.index.load_index(index_path, metadata_path)
        
        # ç¡®ä¿åµŒå…¥æ¨¡å‹å·²åŠ è½½
        if self.embedder.model is None:
            self.embedder.load_model()

# ä¾¿æ·å‡½æ•°
def create_embedding_service(model_name: str = "BAAI/bge-small-zh-v1.5", 
                           chunk_size: int = 550, overlap: int = 100) -> EmbeddingService:
    """åˆ›å»ºåµŒå…¥æœåŠ¡å®ä¾‹"""
    return EmbeddingService(model_name=model_name, chunk_size=chunk_size, overlap=overlap)

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    logging.basicConfig(level=logging.INFO)
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "é¦™æ¸¯åŸå¸‚å¤§å­¦æ˜¯ä¸€æ‰€ä½äºé¦™æ¸¯ä¹é¾™å¡˜çš„å…¬ç«‹ç ”ç©¶å‹å¤§å­¦ã€‚å­¦æ ¡æˆç«‹äº1984å¹´ï¼Œæ˜¯é¦™æ¸¯å…«æ‰€å—æ”¿åºœå¤§å­¦æ•™è‚²èµ„åŠ©å§”å‘˜ä¼šèµ„åŠ©å¹¶å¯é¢æˆå­¦ä½çš„é«˜ç­‰æ•™è‚²é™¢æ ¡ä¹‹ä¸€ã€‚",
        "è®¡ç®—æœºç§‘å­¦ç³»æä¾›æœ¬ç§‘å’Œç ”ç©¶ç”Ÿè¯¾ç¨‹ï¼Œæ¶µç›–äººå·¥æ™ºèƒ½ã€æ•°æ®ç§‘å­¦ã€è½¯ä»¶å·¥ç¨‹ç­‰é¢†åŸŸã€‚å­¦ç”Ÿå¯ä»¥å‚ä¸å„ç§ç ”ç©¶é¡¹ç›®å’Œå®ä¹ æœºä¼šã€‚",
        "æ ¡å›­å†…æœ‰ç°ä»£åŒ–çš„å›¾ä¹¦é¦†ã€å®éªŒå®¤å’Œä½“è‚²è®¾æ–½ã€‚é‚µé€¸å¤«å›¾ä¹¦é¦†æ˜¯å­¦ç”Ÿå­¦ä¹ å’Œç ”ç©¶çš„é‡è¦åœºæ‰€ï¼Œæä¾›ä¸°å¯Œçš„å­¦æœ¯èµ„æºã€‚"
    ]
    
    print("ğŸ”§ åµŒå…¥æœåŠ¡æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºæœåŠ¡
    service = create_embedding_service()
    
    # æ„å»ºç´¢å¼•
    print("ğŸ“š æ„å»ºç´¢å¼•...")
    index = service.build_index_from_texts(test_texts)
    
    # æµ‹è¯•æœç´¢
    queries = ["å›¾ä¹¦é¦†åœ¨å“ªé‡Œ", "è®¡ç®—æœºä¸“ä¸š", "é¦™æ¸¯åŸå¸‚å¤§å­¦"]
    
    for query in queries:
        print(f"\nğŸ” æŸ¥è¯¢: {query}")
        results = service.search(query, top_k=2, min_similarity=0.3)
        
        for i, result in enumerate(results, 1):
            print(f"  [{i}] ç›¸ä¼¼åº¦: {result.similarity:.3f}")
            print(f"      æ–‡æœ¬: {result.text[:100]}...")
            print(f"      æ¥æº: {result.source}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ!")